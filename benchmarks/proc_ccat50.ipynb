{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "345b5ab5-c009-4851-8177-ecae4d1d0b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/scr/oshaikh/miniconda3/envs/handbook/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "modified from the script found here:\n",
    "    https://github.com/bmurauer/authbench/blob/main/scripts/unify_c50.py\n",
    "\n",
    "dataset access:\n",
    "    https://archive.ics.uci.edu/ml/datasets/Reuter_50_50\n",
    "\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from typing import List, Dict\n",
    "import re\n",
    "\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters\n",
    "\n",
    "def process_c50(pth, seed=0):\n",
    "    processed_dir = os.path.join(pth, \"processed\")\n",
    "    if not os.path.isdir(processed_dir):\n",
    "        os.makedirs(processed_dir)\n",
    "    raw_dir = pth\n",
    "\n",
    "    train = os.path.join(raw_dir, \"C50train\")\n",
    "    test = os.path.join(raw_dir, \"C50test\")\n",
    "\n",
    "    def read(subdir: str, author_ids: dict) -> List[Dict]:\n",
    "        posts = {}\n",
    "        authors = os.listdir(subdir)\n",
    "        for j, author in enumerate(authors):\n",
    "            author_dir = os.path.join(subdir, author)\n",
    "            files = glob(author_dir + \"/*.txt\")\n",
    "            for f in files:\n",
    "                with open(f) as i_f:\n",
    "                    text = i_f.read()\n",
    "                    posts.setdefault(author_ids.setdefault(author, j), []).append(text)\n",
    "        return posts, author_ids\n",
    "\n",
    "    logging.info('getting train and test sets')\n",
    "    auth_to_id = {}  # make sure author id's are consistent across train and test set\n",
    "    train_and_eval_dict, auth_to_id = read(train, auth_to_id)\n",
    "    test_dict, auth_to_id = read(test, auth_to_id)\n",
    "\n",
    "    # make a dict of all data for stat tracking\n",
    "    all_data = {}\n",
    "    for data in [train_and_eval_dict, test_dict]:\n",
    "        for k, v in data.items():\n",
    "            for t in v:\n",
    "                all_data.setdefault(k, []).append(t)\n",
    "\n",
    "    # we need to split the train into a training and evaluation set\n",
    "    train_and_eval_data = []\n",
    "    for auth in train_and_eval_dict.keys():\n",
    "        for text in train_and_eval_dict[auth]:\n",
    "            train_and_eval_data.append([auth, text])\n",
    "\n",
    "    logging.info(f'splitting the training data into train/eval sets')\n",
    "    # now split into stratified train(60%)/val(20%)/test(20%) splits\n",
    "    train_set, eval_set = train_test_split(train_and_eval_data, test_size=0.2, shuffle=True, random_state=seed,\n",
    "                                                    stratify=[lbl for lbl, _ in train_and_eval_data])\n",
    "\n",
    "    # now transform back to dicts\n",
    "    train_dict = {}\n",
    "    for auth, text in train_set:\n",
    "        train_dict.setdefault(auth, []).append(text)\n",
    "\n",
    "    val_dict = {}\n",
    "    for auth, text in eval_set:\n",
    "        val_dict.setdefault(auth, []).append(text)\n",
    "\n",
    "    return train_dict, val_dict, test_dict\n",
    "\n",
    "\n",
    "dataset_path = \"./ccat50/\"\n",
    "seed = 0\n",
    "output_path = \"./ccat50/processed\"\n",
    "\n",
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "args = Namespace(dataset_path=dataset_path, seed=seed)\n",
    "\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "train_data, val_data, test_data = process_c50(args.dataset_path, args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "974d8a19-116e-4c05-82eb-228c6773a4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "\n",
    "def split_sentence(sentence):\n",
    "    # Create a Punkt tokenizer with custom parameters\n",
    "    punkt_param = PunktParameters()\n",
    "    abbreviation = ['corp', 'co', \"u.s\", \"inc\", \"cos\", \"u.k\", \"st\"]\n",
    "    punkt_param.abbrev_types = set(abbreviation)\n",
    "    tokenizer = PunktSentenceTokenizer(punkt_param)\n",
    "    \n",
    "    # Tokenize the sentence using the Punkt tokenizer\n",
    "    sentences = tokenizer.tokenize(sentence)\n",
    "    \n",
    "    if len(sentences) > 1:\n",
    "        first_part = sentences[0]\n",
    "        second_part = ' '.join(sentences[1:])\n",
    "        return first_part, second_part\n",
    "    else:\n",
    "        return sentence, ''\n",
    "\n",
    "def promptify(dset_dict):\n",
    "    all_outs = {}\n",
    "    for author in tqdm(dset_dict):\n",
    "        if author not in all_outs:\n",
    "            all_outs[author] = []\n",
    "        for text in dset_dict[author]:\n",
    "            # try:\n",
    "            first_sent, rem = split_sentence(text)\n",
    "\n",
    "            curr_dict = {\n",
    "                \"prompt\": f\"Write an article that starts with the following: {first_sent.strip()}\",\n",
    "                \"output\": text.strip()\n",
    "            }\n",
    "            \n",
    "            # print(curr_dict)\n",
    "\n",
    "            curr_dict[\"output\"] = re.sub(' +', ' ', curr_dict[\"output\"])\n",
    "            curr_dict[\"output\"] = re.sub('\\t', '', curr_dict[\"output\"])\n",
    "            curr_dict[\"output\"] = re.sub('\\r', '', curr_dict[\"output\"])\n",
    "            curr_dict[\"output\"] = re.sub('\\xa0', '', curr_dict[\"output\"])\n",
    "            curr_dict[\"output\"] = ' '.join(curr_dict[\"output\"].split())\n",
    "\n",
    "            all_outs[author].append(curr_dict)\n",
    "                \n",
    "\n",
    "    return all_outs\n",
    "\n",
    "def write_aa_dataset(data: Dict, file_path: str) -> None:\n",
    "    # Save JSON data as PKL\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d46b814b-bed9-4f62-b3a4-8570a3ad7dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 50/50 [00:01<00:00, 29.24it/s]\n",
      "100%|█████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 165.67it/s]\n",
      "100%|██████████████████████████████████████████████████████████| 50/50 [00:01<00:00, 30.83it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_procs = [\n",
    "    (train_data, \"train\"), \n",
    "    (val_data, \"val\"), \n",
    "    (test_data, \"test\"), \n",
    "]\n",
    "\n",
    "for dataset, name in dataset_procs:\n",
    "    \n",
    "    curr_dataset = promptify(dataset)\n",
    "\n",
    "    if name in [\"val\", \"test\"]:\n",
    "        for k in curr_dataset:\n",
    "            curr_dataset[k] = curr_dataset[k][:3]\n",
    "            \n",
    "    write_aa_dataset(curr_dataset, output_path + f\"/ccat50_{name}.pkl\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fc5438a4-8f4c-4176-9cfb-dd3584487913",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handbook",
   "language": "python",
   "name": "handbook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
