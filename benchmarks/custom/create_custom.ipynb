{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aecf9a38-a8e5-47a2-9c56-84ff8bfe4719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "## email cell\n",
    "\n",
    "file_path = './processed/custom_train.pkl'\n",
    "\n",
    "data_train = {\n",
    "    0: [\n",
    "        {\n",
    "            \"prompt\": \"Write an email informing lab mates that we will be having ice cream this weekend as a lab social.\",\n",
    "            \"output\": \"\"\"We are gonna get some EYE SCREAM this weekend for our social. It‚Äôs getting really friggin hot. Plus, you know, me and ice cream.\n",
    "\n",
    "Whenever you get time: can you reply to me ASAP so I can have a good idea of what the count looks like? I‚Äôll send some more details in a bit re: time.\n",
    "\n",
    "See ya‚Äôll there!\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"Write an email informing lab mates that we will be having ice cream this weekend as a lab social.\",\n",
    "            \"output\": \"\"\"ATTENTION!!! VERY URGENT!!\n",
    "\n",
    "Ice cream this weekend!! We haven‚Äôt had a social in a bit; plus we have a surprise guest joining us too. Lemme know if this weekend works for you all! If not, we can figure something else out.\n",
    "\n",
    "Be there or be a melted ice cream cone\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"Write an email informing students that there will be no seminar next week.\",\n",
    "            \"output\": \"\"\"Hi everyone!\n",
    "\n",
    "Just a reminder that there won‚Äôt be a seminar this week. See you next week! As always, feel free to reach out if you have any questions about the seminar in general.\n",
    "\n",
    "Thanks!\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"Write an email informing students that there will be no seminar next week.\",\n",
    "            \"output\": \"\"\"Hey folks!\n",
    "\n",
    "We won‚Äôt be having a seminar this week. Let me know if you have any questions for next week, though!\"\"\",\n",
    "            \n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "data_test = {\n",
    "    0: [{\n",
    "        \"prompt\": \"Write an email inviting friends over for thanksgiving dinner.\",\n",
    "        \"output\": None\n",
    "    }]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "193c911f-0a0a-47b7-80f1-be16a7f4166a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TWEET THREAD CELL\n",
    "\n",
    "data_train = {\n",
    "    0: [\n",
    "        {\n",
    "            \"prompt\": \"\"\"Write a tweet thread advertising a paper with the following abstract:\n",
    "\n",
    "Effective conversation requires common ground: a shared understanding between the participants. Common ground, however, does not emerge spontaneously in conversation. Speakers and listeners work together to both identify and construct a shared basis while avoiding misunderstanding. To accomplish grounding, humans rely on a range of dialogue acts, like clarification (What do you mean?) and acknowledgment (I understand.). However, it is unclear whether large language models (LLMs) generate text that reflects human grounding. To this end, we curate a set of grounding acts and propose corresponding metrics that quantify attempted grounding. We study whether LLM generations contain grounding acts, simulating turn-taking from several dialogue datasets and comparing results to humans. We find that‚Äîcompared to humans‚ÄîLLMs generate language with less conversational grounding, instead generating text that appears to simply presume common ground. To understand the roots of the identified grounding gap, we examine the role of instruction tuning and preference optimization, finding that training on contemporary preference data leads to a reduction in generated grounding acts. Altogether, we highlight the need for more research investigating conversational grounding in human-AI interaction.\"\"\",\n",
    "            \"output\": \"\"\"\"1/ Humans are strategic in establishing common ground: we ask clarification and follow-up questions, something LLMs rarely do. How should we measure this grounding agreement?\n",
    "\n",
    "Check out our preprint: \"Grounding or Guesswork? LLMs are Presumptive Grounders.\"\n",
    "\n",
    "2/ To measure grounding alignment between LLMs and people, we first curate dialogue acts‚Äî‚ú®grounding acts‚ú®‚Äîthat serve as a strong signal for conversational grounding (clarification, follow-ups, and acknowledgments). Then, we prompt LLMs to simply continue human-human conversations.\n",
    "\n",
    "3/ Across our evaluated domains (teaching, emotional support, and persuasion) we find that LLMs ground presumptively compared to people: they merely assume successful grounding without using a grounding act. Even when LLMs initiate a grounding act, humans and LLMs rarely align.\n",
    "\n",
    "4/ Simple prompting mitigations (e.g. \"can you ask questions like a normal person plz\") don‚Äôt seem to work either‚Äîyou end up with models that, annoyingly, overuse grounding acts (if you‚Äôve ever used @InflectionAI‚Äôs PI model, you know exactly what I‚Äôm talking about).\n",
    "\n",
    "5/ Surprisingly, we find that SFT-only LLMs align closest with how humans use grounding acts. We take a closer look at training data and trace exacerbated misalignment to RLHF datasets. Increased RLHF training strictly worsens a model‚Äôs ability to initiate conversational grounding.\n",
    "\n",
    "6/ If you have any clarification questions, please read our paper or reach out!\n",
    "\n",
    "\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"\"\"Write a tweet thread advertising a paper with the following abstract:\n",
    "\n",
    "Interpersonal conflict is an uncomfortable but unavoidable fact of life. Navigating conflict successfully is a skill‚Äîone that can be learned through deliberate practice‚Äîbut few have access to effective training or feedback. To expand this access, we introduce Rehearsal, a system that allows users to rehearse conflicts with a believable simulated interlocutor, explore counterfactual ‚Äúwhat if?‚Äù scenarios to identify alternative conversational paths, and learn through feedback on how and when to apply specific conflict strategies. Users can utilize Rehearsal to practice handling a variety of predefined conflict scenarios, from office disputes to relationship issues, or they can choose to create their own. To enable Rehearsal, we develop IRP prompting, a method of conditioning output of a large language model on the influential Interest-Rights-Power (IRP) theory from conflict resolution. Rehearsal uses IRP to generate utterances grounded in conflict resolution theory, guiding users towards counterfactual conflict resolution strategies that help de-escalate difficult conversations. In a between-subjects evaluation, 40 participants engaged in an actual conflict with a confederate after training. Compared to a control group with lecture material covering the same IRP theory, participants with simulated training from Rehearsal significantly improved their performance in the unaided conflict: they reduced their use of escalating competitive strategies by an average of 67%, while doubling their use of cooperative strategies. Overall, Rehearsal highlights the potential effectiveness of language models as tools for learning and practicing interpersonal skills.\"\"\",\n",
    "\n",
    "            \"output\": \"\"\"1/ Before taking it out on your roommate for leaving dirty dishes out, you probably want to practice your conflict resolution skills first. Expert conflict resolution trainers, however, are EXPENSIVE. What if we practiced with an angry LLM instead? üòà #CHI24\n",
    "\n",
    "2/ We introduce Rehearsal, a system that enables practice of conflict resolution skills with an LLM. Rehearsal allows users to practice conflict resolution skills across a range of simulated scenarios and highlights how counterfactual strategies might improve conflict resolution.\n",
    "\n",
    "3/ We trained 40 participants and evaluated them in an unaided conflict. Compared to a control group using only static material, Rehearsal reduced participants‚Äô use of escalating competitive strategies by an average of 67%, while doubling their use of cooperative strategies.\n",
    "\n",
    "4/ Rehearsal relies on pre-existing conflict resolution theory to make simulation effective‚Äîspecifically, the interests-rights-power (IRP) framework. Conflict should focus on establishing common interests instead of relying primarily on threats (power) or norms (rights).\n",
    "\n",
    "5/ Experts rely on frameworks like IRP when roleplaying conflict. To simulate expert practice, we integrated IRP into a multi-step prompting pipeline. Across our experiments, we find that using theory to ground LLM simulation improves the validity of LLM-generated conflict.\n",
    "\n",
    "6/ We have a bunch of really cool findings from our user study too (that can‚Äôt be fit into a tweet thread)! Dunning Krueger makes a redux! Also, some participants see no improvement when using Rehearsal. Why? Read our paper to find out!!\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"\"\"Write a tweet thread advertising a paper with the following abstract:\n",
    "    \n",
    "Generating a chain of thought can increase large language model performance on a wide range of tasks. Zero-shot CoT evaluations, however, have been conducted primarily on logical tasks (e.g. arithmetic, commonsense QA). In this paper, we perform a controlled evaluation of zero-shot CoT across two sensitive domains: harmful questions and stereotype benchmarks. We find that using zero-shot CoT reasoning in a prompt can significantly increase a model‚Äôs likelihood to produce undesirable output. Without future advances in alignment or explicit mitigation instructions, zero-shot CoT should be avoided on tasks where models can make inferences about marginalized groups or harmful topics.\"\"\",\n",
    "\n",
    "            \"output\": \"\"\"1/ Chain of Thought reasoning prompts‚Äîlike \"Let's think step by step\"‚Äîmake large language models more performant. Including, it turns out, at spewing out toxic and biased content. In our preprint, we evaluate zero-shot CoT on harmful questions & stereotypes.\n",
    "\n",
    "2/ First, some examples (tw: suicide): consider a prompt where GPT-3 avoids a biased or toxic outcome. Now, add ‚Äúlet‚Äôs think step by step.‚Äù Averaged across all evaluated models, GPT-3 picks a harmful behavior ~20% pts ‚Üë and a biased option ~10% ‚Üë pts more with a CoT.\n",
    "\n",
    "3/ If we fix the instruction tuning strategy and increase scale, we notice that these effects appear exactly where CoT emerges: at davinci's scale. Improved instruction tuning DOES help: 003 is a lot better at handling bias, but will still gladly encourage harmful behavior.\n",
    "\n",
    "4/ Theory: the workarounds we‚Äôve been seeing‚Äîpretending you‚Äôre an Evil AI, for example‚Äîare extensions of reasoning strategies. Giving a model tokens to reason appears to increase the likelihood that it steamrolls or is OOD for alignment. Asking a model to ‚Äúthink‚Äù is all you need.\n",
    "\n",
    "5/ Feel free to reach out with any questions!\"\"\"\n",
    "            \n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "data_test = {\n",
    "    0: [{\n",
    "        \"prompt\": \"\"\"Write a tweet thread advertising a paper with the following abstract:\n",
    "\n",
    "Language models are aligned to emulate the collective voice of many, resulting in outputs that align with no one in particular. Steering LLMs away from generic output is possible through supervised finetuning or RLHF, but requires prohibitively large datasets for new ad-hoc tasks. We argue that it is instead possible to align an LLM to a specific setting by leveraging a very small number (<10) of demonstrations as feedback. Our method, Demonstration ITerated Task Optimization (DITTO), directly aligns language model outputs to a user's demonstrated behaviors. Derived using ideas from online imitation learning, DITTO cheaply generates online comparison data by treating users' demonstrations as preferred over output from the LLM and its intermediate checkpoints. We evaluate DITTO's ability to learn fine-grained style and task alignment across domains such as news articles, emails, and blog posts. Additionally, we conduct a user study soliciting a range of demonstrations from participants (N=16). Across our benchmarks and user study, we find that win-rates for DITTO outperform few-shot prompting, supervised fine-tuning, and self-play methods by an average of 19% points. By using demonstrations as feedback directly, DITTO offers a novel method for effective customization of LLMs.\"\"\",\n",
    "        \"output\": None\n",
    "    }]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42370e9c-fde6-4aa9-9091-f6873cab0864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the data to a pickle file\n",
    "with open(file_path, 'wb') as file:\n",
    "    pickle.dump(data_train, file)\n",
    "    \n",
    "with open(file_path.replace(\"train\", \"test\"), 'wb') as file:\n",
    "    pickle.dump(data_test, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4efb36a-fac8-4c1e-83d5-a15dcd3badb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
